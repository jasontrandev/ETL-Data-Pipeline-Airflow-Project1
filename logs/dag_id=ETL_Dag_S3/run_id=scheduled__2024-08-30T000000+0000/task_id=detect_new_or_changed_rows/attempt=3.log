[2024-08-31T11:28:31.941+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Dag_S3.detect_new_or_changed_rows scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-08-31T11:28:31.959+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Dag_S3.detect_new_or_changed_rows scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-08-31T11:28:31.960+0000] {taskinstance.py:2170} INFO - Starting attempt 3 of 8
[2024-08-31T11:28:31.987+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): detect_new_or_changed_rows> on 2024-08-30 00:00:00+00:00
[2024-08-31T11:28:32.001+0000] {standard_task_runner.py:60} INFO - Started process 1821 to run task
[2024-08-31T11:28:32.007+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'ETL_Dag_S3', 'detect_new_or_changed_rows', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '171', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag_s3.py', '--cfg-path', '/tmp/tmp89eq0hvr']
[2024-08-31T11:28:32.011+0000] {standard_task_runner.py:88} INFO - Job 171: Subtask detect_new_or_changed_rows
[2024-08-31T11:28:32.094+0000] {task_command.py:423} INFO - Running <TaskInstance: ETL_Dag_S3.detect_new_or_changed_rows scheduled__2024-08-30T00:00:00+00:00 [running]> on host 3253e99ed5f2
[2024-08-31T11:28:32.270+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='jasontran' AIRFLOW_CTX_DAG_ID='ETL_Dag_S3' AIRFLOW_CTX_TASK_ID='detect_new_or_changed_rows' AIRFLOW_CTX_EXECUTION_DATE='2024-08-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='3' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-08-30T00:00:00+00:00'
[2024-08-31T11:28:32.307+0000] {credentials.py:1147} INFO - Found credentials in environment variables.
[2024-08-31T11:28:33.010+0000] {logging_mixin.py:188} INFO - Succesfully read data from AWS S3
[2024-08-31T11:28:33.019+0000] {connection.py:370} INFO - Snowflake Connector for Python Version: 3.6.0, Python Version: 3.8.18, Platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.34
[2024-08-31T11:28:33.023+0000] {connection.py:1171} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2024-08-31T11:28:33.432+0000] {logging_mixin.py:188} INFO - Connected to Snowflake DWH succesfully
[2024-08-31T11:28:33.433+0000] {cursor.py:1028} INFO - query: [select * from ***_db.tcops_dimensions.employee_dim]
[2024-08-31T11:28:33.543+0000] {cursor.py:1041} INFO - query execution done
[2024-08-31T11:28:33.544+0000] {cursor.py:1205} INFO - Number of results in first chunk: 0
[2024-08-31T11:28:33.545+0000] {logging_mixin.py:188} INFO - Query executed
[2024-08-31T11:28:33.589+0000] {logging_mixin.py:188} INFO - Data fetched
[2024-08-31T11:28:33.591+0000] {logging_mixin.py:188} INFO - Columns renamed
[2024-08-31T11:28:33.608+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/includes/emp_dim_insert_update.py", line 69, in detect_new_or_changed_rows
    result_list = new_inserts[cols_to_insert].values.tolist()
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/frame.py", line 3767, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 5877, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 5941, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['employee_surrogate_key'] not in index"
[2024-08-31T11:28:33.656+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Dag_S3, task_id=detect_new_or_changed_rows, execution_date=20240830T000000, start_date=20240831T112831, end_date=20240831T112833
[2024-08-31T11:28:33.687+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 171 for task detect_new_or_changed_rows ("['employee_surrogate_key'] not in index"; 1821)
[2024-08-31T11:28:33.751+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-08-31T11:28:33.846+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
