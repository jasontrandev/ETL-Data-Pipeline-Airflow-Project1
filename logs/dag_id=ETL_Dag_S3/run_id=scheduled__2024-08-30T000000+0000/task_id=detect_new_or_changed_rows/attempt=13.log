[2024-08-31T16:20:42.954+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_Dag_S3.detect_new_or_changed_rows scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-08-31T16:20:42.971+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_Dag_S3.detect_new_or_changed_rows scheduled__2024-08-30T00:00:00+00:00 [queued]>
[2024-08-31T16:20:42.972+0000] {taskinstance.py:2170} INFO - Starting attempt 13 of 18
[2024-08-31T16:20:43.001+0000] {taskinstance.py:2191} INFO - Executing <Task(_PythonDecoratedOperator): detect_new_or_changed_rows> on 2024-08-30 00:00:00+00:00
[2024-08-31T16:20:43.009+0000] {standard_task_runner.py:60} INFO - Started process 3912 to run task
[2024-08-31T16:20:43.015+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'ETL_Dag_S3', 'detect_new_or_changed_rows', 'scheduled__2024-08-30T00:00:00+00:00', '--job-id', '232', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag_s3.py', '--cfg-path', '/tmp/tmp8u89aaag']
[2024-08-31T16:20:43.019+0000] {standard_task_runner.py:88} INFO - Job 232: Subtask detect_new_or_changed_rows
[2024-08-31T16:20:43.116+0000] {task_command.py:423} INFO - Running <TaskInstance: ETL_Dag_S3.detect_new_or_changed_rows scheduled__2024-08-30T00:00:00+00:00 [running]> on host b5c60e3907e4
[2024-08-31T16:20:43.321+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='jasontran' AIRFLOW_CTX_DAG_ID='ETL_Dag_S3' AIRFLOW_CTX_TASK_ID='detect_new_or_changed_rows' AIRFLOW_CTX_EXECUTION_DATE='2024-08-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='13' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-08-30T00:00:00+00:00'
[2024-08-31T16:20:43.366+0000] {credentials.py:1147} INFO - Found credentials in environment variables.
[2024-08-31T16:20:44.415+0000] {logging_mixin.py:188} INFO - Succesfully read data from AWS S3
[2024-08-31T16:20:44.419+0000] {connection.py:370} INFO - Snowflake Connector for Python Version: 3.6.0, Python Version: 3.8.18, Platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.34
[2024-08-31T16:20:44.421+0000] {connection.py:1171} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2024-08-31T16:20:45.280+0000] {logging_mixin.py:188} INFO - Connected to Snowflake DWH succesfully
[2024-08-31T16:20:45.284+0000] {cursor.py:1028} INFO - query: [select * from ***_db.tcops_dimensions.employee_dim]
[2024-08-31T16:20:45.458+0000] {cursor.py:1041} INFO - query execution done
[2024-08-31T16:20:45.459+0000] {cursor.py:1205} INFO - Number of results in first chunk: 0
[2024-08-31T16:20:45.460+0000] {logging_mixin.py:188} INFO - Query executed
[2024-08-31T16:20:45.510+0000] {logging_mixin.py:188} INFO - Data fetched
[2024-08-31T16:20:45.512+0000] {logging_mixin.py:188} INFO - Columns renamed
[2024-08-31T16:20:45.539+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/decorators/base.py", line 241, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/includes/emp_dim_insert_update.py", line 80, in detect_new_or_changed_rows
    result_list = new_inserts[cols_to_insert].map(escape_single_quotes).values.tolist()
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 5989, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'map'
[2024-08-31T16:20:45.585+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_Dag_S3, task_id=detect_new_or_changed_rows, execution_date=20240830T000000, start_date=20240831T162042, end_date=20240831T162045
[2024-08-31T16:20:45.618+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 232 for task detect_new_or_changed_rows ('DataFrame' object has no attribute 'map'; 3912)
[2024-08-31T16:20:45.685+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-08-31T16:20:45.730+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
